{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一. Model Representation\n",
    "\n",
    "在给定训练集的情况下，学习函数h：X→Y，使得h（x）是y的相应值的“好”预测器。由于历史原因，这个函数h被称为假设。\n",
    "\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/H6qTdZmYEeaagxL7xdFKxA_2f0f671110e8f7446bb2b5b2f75a8874_Screenshot-2016-10-23-20.14.58.png?expiry=1520812800000&hmac=_RVGu2JUuidIVW13kchbboKgBuSg4f9NF1TW5tOlrJI)\n",
    "\n",
    "\n",
    "通过输入住房面积 x，通过学习好的函数，输出房子的估价。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "# 二. Cost Function\n",
    "\n",
    "代价函数是线性回归中的一个应用，在线性回归中，要解决的一个问题就是最小化问题。\n",
    "\n",
    "假设在一元线性回归中，在一个训练集中，我们需要找到一条直线能和该训练集中的点最接近。假设直线方程为 \n",
    "\n",
    "$$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$$\n",
    "\n",
    "\n",
    "如何选择 $\\theta_{0}$、$\\theta_{1}$，使得 $h_{\\theta}(x)$ 更接近于训练集 (x,y) ？\n",
    "\n",
    "上述问题可以转换为求 $$ \\rm{CostFunction} = \\rm{F}({\\theta_{0}},{\\theta_{1}}) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\tag{平方误差代价函数}$$  求最小值$$\\min_{{\\theta_{0}} {\\theta_{1}}} \\rm{F}({\\theta_{0},{\\theta_{1}})} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 三. Gradient Descent 梯度下降\n",
    "\n",
    "\n",
    "梯度下降的主要思想：\n",
    "\n",
    "1. 初始化 ${\\theta_{0}}$ 和 ${\\theta_{1}}$ , ${\\theta_{0}}$ = 0 , ${\\theta_{1}}$ = 0\n",
    "2. 不断的改变 ${\\theta_{0}}$ 和 ${\\theta_{1}}$ 值，不断减少 $F({\\theta_{0}},{\\theta_{1}})$ 直至达到最小值（或者局部最小）。\n",
    "\n",
    "\n",
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/68_1.png)\n",
    "\n",
    "\n",
    "想象成下山，如何下山的速度最快？这里涉及到了下山的速度，即步长。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/bn9SyaDIEeav5QpTGIv-Pg_0d06dca3d225f3de8b5a4a7e92254153_Screenshot-2016-11-01-23.48.26.png?expiry=1520812800000&hmac=fLLO2jY4ccqkRtqtkT-fjubQEPUbuyeNhX1AYc-oCCk)\n",
    "\n",
    "有趣的是换旁边一个点，下山，找到的最优解可能就是另一个了。这也是梯度下降的一个特点。它会找到所有的局部最优解出来。\n",
    "\n",
    "梯度下降算法，不断更新：\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\rm{temp}0 &:= {\\theta_{0}} - \\alpha * \\frac{\\partial }{\\partial {\\theta_{0}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} \\tag {第一步} \\\\\n",
    "\\rm{temp}1 &:= {\\theta_{1}} - \\alpha * \\frac{\\partial }{\\partial {\\theta_{1}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} \\tag {第二步} \\\\\n",
    "{\\theta_{0}} &:= \\rm{temp}0 \\tag {第三步} \\\\\n",
    "{\\theta_{1}} &:= \\rm{temp}1 \\tag {第四步} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "直到收敛。注意 ${\\theta_{0}}$ 和 ${\\theta_{1}}$ 值要**同时更新**，**切记不要求一次导更新一次！**\n",
    "\n",
    "\n",
    "$\\alpha$ 被称作为学习速率。\n",
    "\n",
    "\n",
    "![](https://camo.githubusercontent.com/acf4bf18294bd5379abf9cb1485aab86d65dfc04/68747470733a2f2f7773322e73696e61696d672e636e2f6c617267652f303036744e6337396c7931666d676e32336c6e7a6a673330393830676f67736f2e676966)\n",
    "\n",
    "如果 $\\alpha$ 被设置的很小，需要很多次循环才能到底最低点。\n",
    "如果 $\\alpha$ 被设置的很大，来来回回可能就会离最低点越来越远，**会导致无法收敛，甚至发散**。\n",
    "\n",
    "当快要到最低点的时候，梯度下降会越来越慢，因为 $ \\frac{\\partial }{\\partial {\\theta}}$ 越来越小。\n",
    "\n",
    "------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四. Linear Regression 线性回归\n",
    "\n",
    "梯度下降是很常用的算法，它不仅被用在线性回归，还用在线性回归模型、平方误差代价函数中。\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial {\\theta_{j}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} & = \\frac{\\partial }{\\partial {\\theta_{j}}} \\frac{1}{2m}\\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})^2\\\\\n",
    "& = \\frac{\\partial }{\\partial {\\theta_{j}}} \\frac{1}{2m}\\sum_{i = 1}^{m} (\\theta_{0} + {\\theta_{1}}x^{(i)}-y^{(i)})^2 \\\\\n",
    "& = \\frac{1}{2m} \\frac{\\partial }{\\partial {\\theta_{j}}} \\sum_{i = 1}^{m} (\\theta_{0}^{2} + 2\\theta_{0}\\theta_{1}x^{(i)} - 2\\theta_{0}y^{(i)} + \\theta_{1}^2(x^{(i)})^2 - 2\\theta_{1}x^{(i)}y^{(i)} + (y^{(i)})^2) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial {\\theta_{0}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} &= \\frac{1}{2m} \\frac{\\partial }{\\partial {\\theta_{0}}} \\sum_{i = 1}^{m}(2\\theta_{0} + 2\\theta_{1}x^{(i)} - 2y^{(i)}) \\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 1}^{m}(\\theta_{0} + \\theta_{1}x^{(i)} - y^{(i)}) = \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) \\\\\n",
    "\\frac{\\partial }{\\partial {\\theta_{1}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} &=  \\frac{1}{2m} \\frac{\\partial }{\\partial {\\theta_{1}}} \\sum_{i = 1}^{m}(2\\theta_{0}x^{(i)} + 2\\theta_{1}(x^{(i)})^2 - 2x^{(i)}y^{(i)})\\\\\n",
    "&= \\frac{1}{m} \\sum_{i = 1}^{m}(\\theta_{0} + \\theta_{1}x^{(i)} - y^{(i)}) * x^{(i)} = \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "梯度下降算法：\n",
    "\n",
    "\\begin{align*}\n",
    "\\rm{temp}0 &:= {\\theta_{0}} - \\alpha * \\frac{\\partial }{\\partial {\\theta_{0}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} = {\\theta_{0}} - \\alpha * \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) \\tag {第一步} \\\\\n",
    "\\rm{temp}1 &:= {\\theta_{1}} - \\alpha * \\frac{\\partial }{\\partial {\\theta_{1}}}\\rm{F({\\theta_{0}},{\\theta_{1}})} = {\\theta_{1}} - \\alpha * \\frac{1}{m} \\sum_{i = 1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)}) * x^{(i)} \\tag {第二步} \\\\\n",
    "{\\theta_{0}} &:= \\rm{temp}0 \\tag {第三步} \\\\\n",
    "{\\theta_{1}} &:= \\rm{temp}1 \\tag {第四步} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4fc333c86595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[2.5], [3.5], [6.3], [9.9], [9.91], [8.02],\n",
    "                    [4.5], [5.5], [6.23], [7.923], [2.941], [5.02],\n",
    "                    [6.34], [7.543], [7.546], [8.744], [9.674], [9.643],\n",
    "                    [5.33], [5.31], [6.78], [1.01], [9.68],\n",
    "                    [9.99], [3.54], [6.89], [10.9]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[3.34], [3.86], [5.63], [7.78], [10.6453], [8.43],\n",
    "                    [4.75], [5.345], [6.546], [7.5754], [2.35654], [5.43646],\n",
    "                    [6.6443], [7.64534], [7.546], [8.7457], [9.6464], [9.74643],\n",
    "                    [6.32], [6.42], [6.1243], [1.088], [10.342],\n",
    "                    [9.24], [4.22], [5.44], [9.33]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(x_train, y_train, 'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GitHub Repo：[Halfrost-Field](https://github.com/halfrost/Halfrost-Field)\n",
    "> \n",
    "> Follow: [halfrost · GitHub](https://github.com/halfrost)\n",
    ">\n",
    "> Source: [https://github.com/halfrost/Halfrost-Field/blob/master/contents/Model\\_and\\_Cost\\_Function.md](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Model_and_Cost_Function.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
