{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload-images.jianshu.io/upload_images/1194012-db5ca3ef83a4cc2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. Large Margin Classification 大间距分类器\n",
    "\n",
    "\n",
    "支持向量机模型的做法是，即努力将正样本和负样本用最大的间距分开。\n",
    "\n",
    "$\\left \\| u \\right \\|$为 $\\overrightarrow{u}$ 的范数，也就是向量 $\\overrightarrow{u}$ 的欧几里得长度。\n",
    "\n",
    "\n",
    "毕达哥拉斯定理：\n",
    "\n",
    "$$ \\left \\| u \\right \\| = \\sqrt{u_{1}^{2} + u_{2}^{2}}$$\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. Kernels\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. SVMs in Practice\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. Support_Vector_Machines 测试\n",
    "\n",
    "\n",
    "\n",
    "### 1. Question 1\n",
    "\n",
    "Suppose you have trained an SVM classifier with a Gaussian kernel, and it learned the following decision boundary on the training set:\n",
    "\n",
    "\n",
    "![](http://spark-public.s3.amazonaws.com/ml/images/12.1-b.jpg)\n",
    "\n",
    "When you measure the SVM's performance on a cross validation set, it does poorly. Should you try increasing or decreasing C? Increasing or decreasing $\\sigma^{2}$?\n",
    "\n",
    "\n",
    "A. It would be reasonable to try **decreasing** C. It would also be reasonable to try **increasing** $\\sigma^{2}$.\n",
    "\n",
    "B. It would be reasonable to try **increasing** C. It would also be reasonable to try **increasing** $\\sigma^{2}$.\n",
    "\n",
    "C. It would be reasonable to try **increasing** C. It would also be reasonable to try **decreasing** $\\sigma^{2}$.\n",
    "\n",
    "D. It would be reasonable to try **decreasing** C. It would also be reasonable to try **decreasing** $\\sigma^{2}$.\n",
    "\n",
    "解答：A\n",
    "\n",
    "过拟合应该减小 C 和增大 $\\sigma^2$\n",
    "\n",
    "### 2. Question 2\n",
    "\n",
    "The formula for the Gaussian kernel is given by similarity $(x,l^{(1)})=exp(−\\frac{\\left \\| x-l^{(1)} \\right \\|^{2}}{2\\sigma^{2} })$ .\n",
    "\n",
    "The figure below shows a plot of f1=similarity $(x,l^{(1)})$ when $\\sigma^{2} = 1$.\n",
    "\n",
    "\n",
    "![](http://spark-public.s3.amazonaws.com/ml/images/12.2-question.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Which of the following is a plot of f1 when $\\sigma^{2} = 0.25$?\n",
    "\n",
    "\n",
    "A. ![](http://spark-public.s3.amazonaws.com/ml/images/12.2-b.jpg)\n",
    "\n",
    "B. ![](http://spark-public.s3.amazonaws.com/ml/images/12.2-a.jpg)\n",
    "\n",
    "C. ![](http://spark-public.s3.amazonaws.com/ml/images/12.2-d.jpg)\n",
    "\n",
    "D. ![](http://spark-public.s3.amazonaws.com/ml/images/12.2-c.jpg)\n",
    "\n",
    "\n",
    "解答：A\n",
    "\n",
    " $\\sigma^{2} $ 变小图像变瘦高。\n",
    "\n",
    "### 3. Question 3\n",
    "\n",
    "The SVM solves\n",
    "\n",
    "$$min_{\\theta} C \\sum^{m}_{i=1}y^{(i)}cost_{1}(\\theta^{T}x^{(i)})+(1−y^{(i)})cost_{0}(\\theta^{T}x^{(i)})+\\sum^{n}_{j=1}θ^{2}_{j}$$\n",
    "\n",
    "where the functions $cost_0(z)$ and $cost_1(z)$ look like this:\n",
    "\n",
    "\n",
    "The first term in the objective is:\n",
    "\n",
    "$$C \\sum^{m}_{i=1}y^{(i)}cost_{1}(\\theta^{T}x^{(i)})+(1−y^{(i)})cost_{0}(θ^{T}x^{(i)})$$\n",
    "\n",
    "\n",
    "This first term will be zero if two of the following four conditions hold true. Which are the two conditions that would guarantee that this term equals zero?\n",
    "\n",
    "\n",
    "A. For every example with $y^{(i)}=0$, we have that $θ^{T}x(i) \\leqslant 0$.\n",
    "\n",
    "B. For every example with $y^{(i)}=1$, we have that $θ^{T}x(i) \\geqslant 0$.\n",
    "\n",
    "C. For every example with $y^{(i)}=0$, we have that $θ^{T}x(i)\\leqslant−1$.\n",
    "\n",
    "D. For every example with $y^{(i)}=1$, we have that $θ^{T}x(i)\\geqslant 1$.\n",
    "\n",
    "\n",
    "解答：C、D\n",
    "\n",
    "\n",
    "\n",
    "### 4. Question 4\n",
    "\n",
    "Suppose you have a dataset with n = 10 features and m = 5000 examples.\n",
    "\n",
    "After training your logistic regression classifier with gradient descent, you find that it has underfit the training set and does not achieve the desired performance on the training or cross validation sets.\n",
    "\n",
    "Which of the following might be promising steps to take? Check all that apply.\n",
    "\n",
    "\n",
    "A. Try using a neural network with a large number of hidden units.\n",
    "\n",
    "\n",
    "B. Create / add new polynomial features.\n",
    "\n",
    "C. Reduce the number of examples in the training set.\n",
    "\n",
    "D. Use a different optimization method since using gradient descent to train logistic regression might result in a local minimum.\n",
    "\n",
    "解答： A、B\n",
    "\n",
    "题干中要求解决欠拟合的问题。\n",
    "\n",
    "A.增多神经网络的隐藏层可以解决欠拟合问题。  \n",
    "B.增加特征量可以解决欠拟合问题。  \n",
    "C.减少训练集样本，不行。  \n",
    "D.不是梯度下降函数到达最低值是代价函数。  \n",
    "\n",
    "### 5. Question 5\n",
    "\n",
    "Which of the following statements are true? Check all that apply.\n",
    "\n",
    "\n",
    "A. Suppose you have 2D input examples (ie, $x^{(i)} \\in \\mathbb{R}^2$). The decision boundary of the SVM (with the linear kernel) is a straight line.\n",
    "\n",
    "B. If the data are linearly separable, an SVM using a linear kernel will return the same parameters $\\theta$ regardless of the chosen value of C (i.e., the resulting value of $\\theta$ does not depend on C).\n",
    "\n",
    "C. If you are training multi-class SVMs with the one-vs-all method, it is not possible to use a kernel.\n",
    "\n",
    "D. The maximum value of the Gaussian kernel (i.e., $sim(x,l^{(1)})$) is 1.\n",
    "\n",
    "解答：A、D\n",
    "\n",
    "A. 线性是一条直线。  \n",
    "B. $min_{\\theta} C[\\sum_{i=1}^{m}{y^{(i)}}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\frac{1}{2}\\sum_{j=1}^{n}{\\theta_j^2}$， $\\theta$正是由 C 的大小决定的。   \n",
    "C. 解决多分类问题可以用 SVM 。      \n",
    "D. 高斯核函数范围：[0,1]。  \n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GitHub Repo：[Halfrost-Field](https://github.com/halfrost/Halfrost-Field)\n",
    "> \n",
    "> Follow: [halfrost · GitHub](https://github.com/halfrost)\n",
    ">\n",
    "> Source: [https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine\\_Learning/Support\\_Vector\\_Machines.ipynb](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Support_Vector_Machines.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
