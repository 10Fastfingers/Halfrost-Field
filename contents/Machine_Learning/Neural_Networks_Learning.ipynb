{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/72_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. Cost Function and Backpropagation\n",
    "\n",
    "### 1. Cost Function\n",
    "\n",
    "\n",
    "![](https://blog.webkid.io/content/images/old/neural-networks-in-javascript/nn_blog.png)\n",
    "\n",
    "\n",
    "假设训练集中有 m 个训练样本，$\\begin{Bmatrix} (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \\cdots ,(x^{(m)},y^{(m)}) \\end{Bmatrix}$，L 表示神经网络的总层数 Layer，用 $S_{l}$ 表示第 L 层的单元数(神经元的数量)，但是不包括第 L 层的偏差单元(常数项)。令 K 为输出层的单元数目，即 最后一层的单元数。\n",
    "\n",
    "符号约定：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_i^{(j)} =& \\text{第$j$层的第$i$个节点（神经元）的“计算值”} \\newline    \n",
    "a_i^{(j)} =& \\text{第$j$层的第$i$个节点（神经元）的“激活值”} \\newline    \n",
    "\\Theta^{(l)}_{i,j} =& \\text{映射第$l$层到第$l+1$层的权值矩阵的第$i$行第$j$列的分量} \\newline    \n",
    "L =& \\text{神经网络总层数（包括输入层、隐层和输出层）} \\newline    \n",
    "s_l =& \\text{第$l$层节点（神经元）个数，不包括偏移量节点。} \\newline     \n",
    "K =& \\text{输出节点个数} \\newline    \n",
    "h_{\\theta}(x)_k =& \\text{第$k$个预测输出结果} \\newline    \n",
    "x^{(i)} =& \\text{第$i$个样本特征向量} \\newline    \n",
    "x^{(i)}_k =& \\text{第$i$个样本的第$k$个特征值} \\newline    \n",
    "y^{(i)} =& \\text{第$i$个样本实际结果向量} \\newline  \n",
    "y^{(i)}_k =& \\text{第$i$个样本结果向量的第$k$个分量} \\newline  \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "之前讨论的逻辑回归中代价函数如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\rm{CostFunction} = \\rm{F}({\\theta}) &= -\\frac{1}{m}\\left [ \\sum_{i=1}^{m} y^{(i)}logh_{\\theta}(x^{(i)}) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)})) \\right ] +\\frac{\\lambda}{2m} \\sum_{j=1}^{n}\\theta_{j}^{2}  \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "扩展到神经网络中：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\rm{CostFunction} = \\rm{F}({\\Theta}) &= -\\frac{1}{m}\\left [ \\sum_{i=1}^{m} \\sum_{k=1}^{K} y^{(i)}_{k} log(h_{\\Theta}(x^{(i)}))_{k} + (1-y^{(i)}_{k})log(1-(h_{\\Theta}(x^{(i)}))_{k}) \\right ] +\\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{S_{l}}\\sum_{j=1}^{S_{l} +1}(\\Theta_{j,i}^{(l)})^{2}  \\\\\n",
    "h_{\\Theta}(x) &\\in \\mathbb{R}^{K} \\;\\;\\;\\;\\;\\;\\;\\;\\; (h_{\\Theta}(x))_{i} = i^{th} \\;\\;output \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$h_{\\Theta}(x)$ 是一个 K 维向量，$ i $ 表示选择输出神经网络输出向量中的第 i 个元素。\n",
    "\n",
    "神经网络的代价函数相比逻辑回归的代价函数，前一项的求和过程中多了一个 $ \\sum_{k=1}^{K} $ ,由于 K 代表了最后一层的单元数，所以这里就是累加了 k 个输出层的代价函数。\n",
    "\n",
    "\n",
    "后一项是正则化项，神经网络的正则化项看起来特别复杂，其实就是对 $ (\\Theta_{j,i}^{(l)})^{2} $ 项对所有的 i，j，l的值求和。正如在逻辑回归中的一样，这里要除去那些对应于偏差值的项，因为我们不对它们进行求和，即不对 $ (\\Theta_{j,0}^{(l)})^{2} \\;\\;\\;\\;(i=0) $ 项求和。\n",
    "\n",
    "### 2. Backpropagation Algorithm 反向传播算法\n",
    "\n",
    "\n",
    "令 $ \\delta_{j}^{(l)} $ 表示第 $l$ 层第 $j$ 个结点的误差。\n",
    "\n",
    "反向传播从最后一层开始往前推：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\delta_{j}^{(L)} &= a_{j}^{(L)} - y_{j} \\\\\n",
    "&=(h_{\\theta}(x))_{j} - y_{j} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### 3. Backpropagation Intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "## 二. Backpropagation in Practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "## 三. Application of Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "## 四. Neural Networks: Learning 测试\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GitHub Repo：[Halfrost-Field](https://github.com/halfrost/Halfrost-Field)\n",
    "> \n",
    "> Follow: [halfrost · GitHub](https://github.com/halfrost)\n",
    ">\n",
    "> Source: [https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural\\_Networks\\_Learning.ipynb](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
