{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. Normal Equation\n",
    "\n",
    "正规方程法相对梯度下降法，它可以一步找到最小值。而且它也不需要进行特征值的缩放。\n",
    "\n",
    "样本集是 $ m * n $ 的矩阵，每行样本表示为 $ \\vec{x^{(i)}} $ ,第 i 行第 n 列分别表示为 $ x^{(i)}_{0} 、x^{(i)}_{1} 、x^{(i)}_{2} 、x^{(i)}_{3} \\cdots x^{(i)}_{n} $, m 行向量分别表示为 $ \\vec{x^{(1)}} 、\\vec{x^{(2)}} 、\\vec{x^{(3)}} 、\\cdots \\vec{x^{(m)}} $\n",
    "\n",
    "令 \n",
    "\n",
    "$$ \\vec{x^{(i)}} = \\begin{bmatrix} x^{(i)}_{0}\\\\ x^{(i)}_{1}\\\\ \\vdots \\\\ x^{(i)}_{n}\\\\ \\end{bmatrix} $$\n",
    "\n",
    "$ \\vec{x^{(i)}} $ 是这样一个 $(n+1)*1$ 维向量。每行都对应着 i 行 0-n 个变量。\n",
    "\n",
    "再构造几个矩阵：\n",
    "\n",
    "$$ X = \\begin{bmatrix} (\\vec{x^{(1)}})^{T}\\\\  \\vdots \\\\  (\\vec{x^{(m)}})^{T} \\end{bmatrix} \\;\\;\\;\\;\n",
    "\\Theta = \\begin{bmatrix} \\theta_{0}\\\\ \\theta_{1}\\\\ \\vdots \\\\ \\theta_{n}\\\\ \\end{bmatrix} \\;\\;\\;\\;\n",
    "Y = \\begin{bmatrix} y^{(1)}\\\\ y^{(2)}\\\\ \\vdots \\\\ y^{(m)}\\\\ \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "X 是一个 $ m * (n+1)$ 的矩阵，$ \\Theta $ 是一个 $ (n+1) * 1$ 的矩阵，Y 是一个 $ m * 1$的矩阵。\n",
    "\n",
    "对比之前代价函数中，$$ \\rm{CostFunction} = \\rm{F}({\\theta_{0}},{\\theta_{1}}) = \\frac{1}{2m}\\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})^2 $$  \n",
    "\n",
    "$$ X \\cdot \\Theta - Y = \\begin{bmatrix}\n",
    "(\\vec{x^{(1)}})^{T}\\\\ \n",
    "\\vdots \\\\ \n",
    "(\\vec{x^{(m)}})^{T}\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix} \\theta_{0}\\\\ \\theta_{1}\\\\ \\vdots \\\\ \\theta_{n}\\\\ \\end{bmatrix} - \\begin{bmatrix} y^{(1)}\\\\ y^{(2)}\\\\ \\vdots \\\\ y^{(m)}\\\\ \\end{bmatrix} = \\begin{bmatrix} h_{\\theta}(x^{(1)})-y^{(1)}\\\\ h_{\\theta}(x^{(2)})-y^{(2)}\\\\ \\vdots \\\\ h_{\\theta}(x^{(m)})-y^{(m)}\\\\ \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "代入到之前代价函数中，\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\rm{CostFunction} = \\rm{F}({\\theta_{0}},{\\theta_{1}}) &= \\frac{1}{2m}\\sum_{i = 1}^{m} (h_{\\theta}(x^{(i)})-y^{(i)})^2\\\\\n",
    "& = \\frac{1}{2m} (X \\cdot \\Theta - Y)^{T}(X \\cdot \\Theta - Y)\\\\\n",
    "\\end{align*}\n",
    "$$  \n",
    "\n",
    "\n",
    "梯度下降和正规方程法比较：\n",
    "\n",
    "优点：\n",
    "梯度下降在超大数据集面前也能运行的很良好。  \n",
    "正规方程在超大数据集合面前性能会变得很差，因为需要计算 $(x^{T}x)^{-1}$,时间复杂度在 $O(n^{3})$ 这个级别。  \n",
    "\n",
    "缺点：\n",
    "梯度下降需要合理的选择学习速率 $\\alpha$ , 需要很多次迭代的操作去选择合理的 $\\alpha$，寻找最小值的时候也需要迭代很多次才能收敛。  \n",
    "正规方程的优势相比而言，不需要选择学习速率 $\\alpha$，也不需要多次的迭代或者画图检测是否收敛。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. Normal Equation Noninvertibility\n",
    "\n",
    "上一章谈到了如何利用正规方程法求解 $\\Theta $,但是在线性代数中存在这样一个问题，如果是奇异(退化)矩阵，是不存在逆矩阵的。也就是说用上面正规方程的公式是不一定能求解出正确结果的。\n",
    "\n",
    "在 Octave 软件中，存在2个求解逆矩阵的函数，一个是 pinv 和 inv。pinv (pseudo-inverse)求解的是**伪逆矩阵**，inv 求解的是逆矩阵，所以用 pinv 求解问题，就算是 $ X^{T}X $ 不存在逆矩阵，也一样可以得到最后的结果。\n",
    "\n",
    "导致$ X^{T}X $ 不存在逆矩阵有2种情况：\n",
    "\n",
    "1. 多余的特征。特征之间呈倍数关系，线性依赖。\n",
    "2. 过多的特征。当 $ m \\leqslant n $ 的时候，会导致过多的特征。解决办法是删除一些特征，或者进行正则化。\n",
    "\n",
    "所以解决$ X^{T}X $ 不存在逆矩阵的办法也就是对应上面2种情况：\n",
    "\n",
    "1. 删掉多余的特征，线性相关的，倍数关系的。直到没有多余的特征\n",
    "2. 再删除一些不影响结果的特征，或者进行正则化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
