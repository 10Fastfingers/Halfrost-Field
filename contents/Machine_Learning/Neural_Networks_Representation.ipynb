{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/71_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. Motivations\n",
    "\n",
    "\n",
    "假如我们用之前的逻辑回归解决以下分类问题：\n",
    "\n",
    "![](https://pic4.zhimg.com/v2-464f99ef2a451491192fab1878856962_r.jpg)\n",
    "\n",
    "\n",
    "我们需要构造一个有很多项的非线性的逻辑回归函数。当只有两个特征量的时候，这还算比较简单的，但是假如我们有100个特征量呢？我们只考虑二阶项的话，其二阶项的个数大约是 $\\frac{n^2}{2}$ 。假如我们要包含所有的二阶项的话这样看起来不是一个好办法，因为项数实在太多运算量也很多，而且最后结果往往容易造成过拟合。当然我们只是考虑了二阶项，考虑二阶项以上的就更多了。\n",
    "\n",
    "当初始特征个数 n 增大时，这些高阶多项式项数将以几何级数上升，特征空间也会随之急剧膨胀 。所以当特征个数 n比较大的时候，用这个方法建立分类器并不是一个好的做法。\n",
    "\n",
    "而对于大多数的机器学习问题， n  一般是比较大的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "## 二. Neural Networks\n",
    "\n",
    "用一个简单的模型来模拟神经元的工作，我们将神经元模拟成一个逻辑单元：\n",
    "\n",
    "![](https://pic4.zhimg.com/v2-406dcccc6374b49b5a4b64085f25bf22_r.jpg)\n",
    "\n",
    "$x_{1},x_{2},x_{3}$ 可以将其看成输入神经树突，黄色的圆圈则可以看成中心处理器细胞核， $h_\\theta(x)$ 则可看成输出神经轴突。因为这里是逻辑单元，所以我们的输出函数为： $h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$ 。一般我们把这称为一个有 s 型函数（逻辑函数）作为激励的人工神经元。\n",
    "\n",
    "\n",
    "那么神经网络其实就是这些神经元组合在一起的集合，如下图：\n",
    "\n",
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/71_2.png)\n",
    "\n",
    "左边第一层 Layer1 被称为**输入层**。在输入层我们输入我们的特征项 $x_{1},x_{2},x_{3}$ 。\n",
    "\n",
    "右边最后一层被称为**输出层**。输出函数为： $h_\\Theta(x)$ 。\n",
    "\n",
    "中间这层被称为**隐藏层**。\n",
    "\n",
    "\n",
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/71_3.png)\n",
    "\n",
    "其中隐藏层中的元素我们用 $a_i^{(j)}$ 表示。上标 j 表示的是第几层（有时候我们并不只有简单一层），下标 i 表示第几个。\n",
    "\n",
    "上面的神经网络可以简单的表示为：\n",
    "\n",
    "\n",
    "$$\\begin{bmatrix} x_{0}\\\\ x_{1}\\\\ x_{2}\\\\ x_{3} \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_{1}^{(2)}\\\\ a_{2}^{(2)}\\\\ a_{3}^{(2)} \\end{bmatrix} \\rightarrow h_{\\theta}(x) $$\n",
    "\n",
    "\n",
    "左边输入层多增加了一个偏置单元(偏置神经元)，$x_{0}$\n",
    "\n",
    "用 $\\Theta^{(j)}$ 表示特征量前的参数，是一个有权重的矩阵控制着一层参数的大小。\n",
    "\n",
    "上述的神经网络可用数学表达，如下：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{1}^{(2)} &= g(\\Theta_{10}^{(1)}x_{0}+\\Theta_{11}^{(1)}x_{1}+\\Theta_{12}^{(1)}x_{2}+\\Theta_{13}^{(1)}x_{3}) \\\\\n",
    "a_{2}^{(2)} &= g(\\Theta_{20}^{(1)}x_{0}+\\Theta_{21}^{(1)}x_{1}+\\Theta_{22}^{(1)}x_{2}+\\Theta_{23}^{(1)}x_{3}) \\\\\n",
    "a_{3}^{(2)} &= g(\\Theta_{30}^{(1)}x_{0}+\\Theta_{31}^{(1)}x_{1}+\\Theta_{32}^{(1)}x_{2}+\\Theta_{33}^{(1)}x_{3}) \\\\\n",
    "h_{\\Theta}(x) &= a_{1}^{(3)} = g(\\Theta_{10}^{(2)}a_{0}^{(2)}+\\Theta_{11}^{(2)}a_{1}^{(2)}+\\Theta_{12}^{(2)}a_{2}^{(2)}+\\Theta_{13}^{(2)}a_{3}^{(2)}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$\\Theta$ 矩阵也被称作为模型的权重。\n",
    "\n",
    "\n",
    "对上面的神经网络数学表达方式进行向量化推导，令：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_{1}^{(2)} &= \\Theta_{10}^{(1)}x_{0}+\\Theta_{11}^{(1)}x_{1}+\\Theta_{12}^{(1)}x_{2}+\\Theta_{13}^{(1)}x_{3} \\\\\n",
    "z_{2}^{(2)} &= \\Theta_{20}^{(1)}x_{0}+\\Theta_{21}^{(1)}x_{1}+\\Theta_{22}^{(1)}x_{2}+\\Theta_{23}^{(1)}x_{3} \\\\\n",
    "z_{3}^{(2)} &= \\Theta_{30}^{(1)}x_{0}+\\Theta_{31}^{(1)}x_{1}+\\Theta_{32}^{(1)}x_{2}+\\Theta_{33}^{(1)}x_{3} \\\\\n",
    "\\vdots \\\\\n",
    "z_{k}^{(2)} &= \\Theta_{k,0}^{(1)}x_{0}+\\Theta_{k,1}^{(1)}x_{1}+\\Theta_{k,2}^{(1)}x_{2}+\\Theta_{k,3}^{(1)}x_{3} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "于是可以得到：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_{1}^{(2)} &= g(z_{1}^{(2)}) \\\\\n",
    "a_{2}^{(2)} &= g(z_{2}^{(2)}) \\\\\n",
    "a_{3}^{(2)} &= g(z_{3}^{(2)}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "用向量即可表示为：\n",
    "\n",
    "$$x = \\begin{bmatrix}\n",
    "x_{0}\\\\ \n",
    "x_{1}\\\\ \n",
    "x_{2}\\\\ \n",
    "x_{3}\n",
    "\\end{bmatrix},z^{(2)} = \\begin{bmatrix}\n",
    "z_{1}^{(2)}\\\\ \n",
    "z_{2}^{(2)}\\\\ \n",
    "z_{3}^{(2)}\\\\ \n",
    "\\end{bmatrix} = \\Theta^{(1)}x$$\n",
    "\n",
    "统一一下前后两层的输入输出关系，将 $x=a^{(1)}$，即可得到：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x &= \\begin{bmatrix}\n",
    "x_{0}\\\\ \n",
    "x_{1}\\\\ \n",
    "\\vdots \\\\ \n",
    "x_{n}\n",
    "\\end{bmatrix},z^{(j)} = \\begin{bmatrix}\n",
    "z_{1}^{(j)}\\\\ \n",
    "z_{2}^{(j)}\\\\\n",
    "\\vdots \\\\ \n",
    "z_{3}^{(j)}\\\\ \n",
    "\\end{bmatrix}, \\\\ \n",
    "\\Rightarrow  z^{(j)} &=\\Theta^{(j-1)}a^{(j-1)}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "这里也可以得到一个结论：\n",
    "\n",
    "假如一个网络里面在第 j  层有 $s_j$ 个单元，在第 j+1 层有 $s_{j+1}$ 个单元，那么 $\\Theta^{(j)}$ 则控制着第 j 层到第 j+1 层的映射矩阵，矩阵的维度是： $s_{j+1} * (s_j + 1)$ 。(例如： j=1 , $s_j=1$， $s_{j+1}$=1 ，也就是说第一层只有一个单元，第二层也只有一个单元，那么 $\\Theta^{(1)}$ 矩阵维度就是 1 * 2 ,因为要算上偏置单元)\n",
    "\n",
    "因为我们通常有 $a_0^{(j)}=1$ ，所以：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "a^{(j)}&=g(z^{(j)})\\\\\n",
    "z^{(j+1)}&=\\Theta^{(j)}a^{(j)}\\\\\n",
    "h_\\Theta(x)&=a^{(j+1)}=g(z^{(j+1)})\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "由这个关系其实可以看出，神经网络跟之前所学的逻辑回归根本区别在于，它是将上一层的输出当做下一层的输入，这个从输入层到隐藏层再到输出层一次计算激励的过程叫做 **forward propagation（前向传播）**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "## 三. Applications\n",
    "\n",
    "### 1. 逻辑运算\n",
    "\n",
    "利用神经网络进行 逻辑与运算\n",
    "\n",
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/71_4.png)\n",
    "\n",
    "\n",
    "利用神经网络进行 逻辑非运算\n",
    "\n",
    "![](https://pic1.zhimg.com/v2-9a3bb99b333dd3e2e36b56e80ae1a930_r.jpg)\n",
    "\n",
    "\n",
    "但是单一一层无法完成异或运算。\n",
    "\n",
    "![](https://pic1.zhimg.com/v2-9572dabf5558fa0c0695878a1bcf4f99_r.jpg)\n",
    "\n",
    "异或在几何上的问题其实是将红叉和蓝圈分开，但是我们的输出函数是： $h_\\Theta(x)=g(\\Theta_{10}^{(1)}x_0+\\Theta_{11}^{(1)}x_1+\\Theta_{12}^{(1)}x_2)$ ,这是线性的，那么在图上无论怎么画一条直线，也没有办法将两个不同的训练集分开。既然一条直线不行，那么神经网络增加一层。\n",
    "\n",
    "![](https://ob6mci30g.qnssl.com/Blog/ArticleImage/71_5.png)\n",
    "\n",
    "\n",
    "如上图，将第二层第一个元素 $a_1^{(2)}$ 作为与运算的结果，第二个元素 $a_2^{(2)}$ 作为或非运算的结果， $a_1^{(2)}$ 和 $a_2^{(2)}$ 再作为输入，进行或运算，作为第三层输出的结果，最后得到的结果与输入的关系正是异或运算的关系。\n",
    "\n",
    "### 2. 本质\n",
    "\n",
    "![](https://pic1.zhimg.com/80/v2-0c5669ca58ad7076ca3a7fdf636796f5_hd.jpg)\n",
    "\n",
    "神经网络正是这样解决比较复杂的函数，当层数很多的时候，我们有一个相对简单的输入量，通过加以权重和不同的运算送到第二层，而第三层在第二层作为输入的基础上再来进行一些更复杂的运算，一层一层下去解决问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "\n",
    "## 四. Neural Networks: Representation 测试\n",
    "\n",
    "\n",
    "### 1. Question 1\n",
    "\n",
    "Which of the following statements are true? Check all that apply.\n",
    "\n",
    "A. Suppose you have a multi-class classification problem with three classes, trained with a 3 layer network. Let a(3)1=(hΘ(x))1 be the activation of the first output unit, and similarly a(3)2=(hΘ(x))2 and a(3)3=(hΘ(x))3. Then for any input x, it must be the case that a(3)1+a(3)2+a(3)3=1.\n",
    "\n",
    "B. The activation values of the hidden units in a neural network, with the sigmoid activation function applied at every layer, are always in the range (0, 1).\n",
    "\n",
    "C. A two layer (one input layer, one output layer; no hidden layer) neural network can represent the XOR function.\n",
    "\n",
    "D. Any logical function over binary-valued (0 or 1) inputs x1 and x2 can be (approximately) represented using some neural network.\n",
    "\n",
    "解答： B、D\n",
    "\n",
    "B.S型函数作为判断函数运用到每一层，其范围是[0,1]，正确。\n",
    "D.任何二进制输入的逻辑运算都可以神经网络解决，正确。\n",
    "C.异或不可以用一层神经网络解决。\n",
    "A.不一定，决策函数不是S型函数的话最后结果相加就不是1了。\n",
    "\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GitHub Repo：[Halfrost-Field](https://github.com/halfrost/Halfrost-Field)\n",
    "> \n",
    "> Follow: [halfrost · GitHub](https://github.com/halfrost)\n",
    ">\n",
    "> Source: [https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural\\_Networks\\_Representation.ipynb](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Machine_Learning/Neural_Networks_Representation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
